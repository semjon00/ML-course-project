{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\n\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-18T09:56:56.138079Z","iopub.execute_input":"2023-12-18T09:56:56.138773Z","iopub.status.idle":"2023-12-18T09:57:08.626743Z","shell.execute_reply.started":"2023-12-18T09:56:56.138744Z","shell.execute_reply":"2023-12-18T09:57:08.625757Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Paths\ntrain_csv = '/kaggle/input/UBC-OCEAN/train.csv'\ntrain_images = '/kaggle/input/UBC-OCEAN/train_images/'\ntrain_thumbnails = '/kaggle/input/UBC-OCEAN/train_thumbnails/'\nmodel_file = '/kaggle/working/model.h5'\n\n\nclasses = ['CC', 'EC', 'HGSC', 'LGSC', 'MC']\n\n# If the best prediction probability is below that threshold, it's labelled as 'Other'\nthreshold = 0.3","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:57:08.628762Z","iopub.execute_input":"2023-12-18T09:57:08.629736Z","iopub.status.idle":"2023-12-18T09:57:08.635164Z","shell.execute_reply.started":"2023-12-18T09:57:08.629697Z","shell.execute_reply":"2023-12-18T09:57:08.633867Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Functions to convert labels to one-hot vectors and vice-versa\ndef values_to_one_hot(values, classes):\n    vector = []\n    for value in values:\n        one_hot = np.zeros(5)\n        one_hot[classes.index(value)] = 1\n        vector.append(one_hot)\n    return np.array(vector)\n\ndef one_hot_to_values(vector, classes):\n    values = []\n    for one_hot in vector:\n        if np.max(one_hot) < threshold:\n            value = 'Other'\n        else:\n            value = classes[np.argmax(one_hot)]\n        values.append(value)\n    return np.array(values)\n\n# Function to load image and preprocess it\ndef load_image(idx):\n    # Using TMAs and WSI thumbnails\n    try:\n        image = Image.open(train_thumbnails+str(idx)+'_thumbnail.png')\n    except:\n        image = Image.open(train_images+str(idx)+'.png')\n    # Resizing to 244x244 for Resnet model\n    image = image.resize((224,224)) \n    image = np.array(image)\n    return image\n    ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-18T09:57:08.636913Z","iopub.execute_input":"2023-12-18T09:57:08.637568Z","iopub.status.idle":"2023-12-18T09:57:08.669705Z","shell.execute_reply.started":"2023-12-18T09:57:08.637530Z","shell.execute_reply":"2023-12-18T09:57:08.668723Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Reading csv file\ntraining_df = pd.read_csv(train_csv)\nn = len(training_df)\n\n# Loading all training images \nimage_ids = []\nimages = []\nlabels = []\nfor i, (idx, label) in enumerate(zip(training_df['image_id'], training_df['label'])):\n    image = load_image(idx)\n    image_ids.append(idx)\n    images.append(image)\n    labels.append(label)\n    print(f'Loading images: {i+1} / {n}',end='\\r')\n\n# Converting labels to one-hot vectors \nlabels_one_hot = values_to_one_hot(labels, classes)\n# Reshaping image array for model training\nimages = np.array(images).reshape(-1, 224, 224, 3)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T09:57:08.671563Z","iopub.execute_input":"2023-12-18T09:57:08.671991Z","iopub.status.idle":"2023-12-18T09:59:40.642400Z","shell.execute_reply.started":"2023-12-18T09:57:08.671944Z","shell.execute_reply":"2023-12-18T09:59:40.641335Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Loading images: 538 / 538\r","output_type":"stream"}]},{"cell_type":"code","source":"# Loading pre-trained ResNet50 model\n# adding a final layer to change the number of output classes\nbase_model = ResNet50(weights='imagenet', include_top=False)\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\npredictions = Dense(len(classes), activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Not changing pre-trained layers\nfor layer in base_model.layers:\n  layer.trainable = False\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train-valitation split\nX_train, X_val, y_train, y_val = train_test_split(images, labels_one_hot, test_size=0.3)\n\n# Training model\nmodel.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_val, y_val))\n\n# Predicting validation set labels\npredicted = one_hot_to_values(model.predict(X_val), classes)\ny_val = one_hot_to_values(y_val, classes)\n\nprint(f'Balanced accuracy: {balanced_accuracy_score(y_val, predicted)}')","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:10:50.817933Z","iopub.execute_input":"2023-12-18T10:10:50.818857Z","iopub.status.idle":"2023-12-18T10:11:16.102112Z","shell.execute_reply.started":"2023-12-18T10:10:50.818820Z","shell.execute_reply":"2023-12-18T10:11:16.101175Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch 1/20\n6/6 [==============================] - 5s 373ms/step - loss: 1.9136 - accuracy: 0.2739 - val_loss: 1.5964 - val_accuracy: 0.3272\nEpoch 2/20\n6/6 [==============================] - 1s 155ms/step - loss: 1.5488 - accuracy: 0.3644 - val_loss: 1.5943 - val_accuracy: 0.3827\nEpoch 3/20\n6/6 [==============================] - 1s 153ms/step - loss: 1.3820 - accuracy: 0.4122 - val_loss: 1.4655 - val_accuracy: 0.3889\nEpoch 4/20\n6/6 [==============================] - 1s 154ms/step - loss: 1.2543 - accuracy: 0.4601 - val_loss: 1.4188 - val_accuracy: 0.4506\nEpoch 5/20\n6/6 [==============================] - 1s 154ms/step - loss: 1.1597 - accuracy: 0.5106 - val_loss: 1.4458 - val_accuracy: 0.3765\nEpoch 6/20\n6/6 [==============================] - 1s 154ms/step - loss: 1.0960 - accuracy: 0.5559 - val_loss: 1.3847 - val_accuracy: 0.4568\nEpoch 7/20\n6/6 [==============================] - 1s 157ms/step - loss: 1.0407 - accuracy: 0.5851 - val_loss: 1.3848 - val_accuracy: 0.4259\nEpoch 8/20\n6/6 [==============================] - 1s 154ms/step - loss: 1.0030 - accuracy: 0.6250 - val_loss: 1.3514 - val_accuracy: 0.4630\nEpoch 9/20\n6/6 [==============================] - 1s 155ms/step - loss: 0.9525 - accuracy: 0.6436 - val_loss: 1.3618 - val_accuracy: 0.4321\nEpoch 10/20\n6/6 [==============================] - 1s 155ms/step - loss: 0.9077 - accuracy: 0.6755 - val_loss: 1.3474 - val_accuracy: 0.4630\nEpoch 11/20\n6/6 [==============================] - 1s 153ms/step - loss: 0.8758 - accuracy: 0.6702 - val_loss: 1.3685 - val_accuracy: 0.4321\nEpoch 12/20\n6/6 [==============================] - 1s 155ms/step - loss: 0.8345 - accuracy: 0.7048 - val_loss: 1.3516 - val_accuracy: 0.4630\nEpoch 13/20\n6/6 [==============================] - 1s 154ms/step - loss: 0.8137 - accuracy: 0.6995 - val_loss: 1.3664 - val_accuracy: 0.4383\nEpoch 14/20\n6/6 [==============================] - 1s 154ms/step - loss: 0.7860 - accuracy: 0.7473 - val_loss: 1.3678 - val_accuracy: 0.4877\nEpoch 15/20\n6/6 [==============================] - 1s 154ms/step - loss: 0.7574 - accuracy: 0.7287 - val_loss: 1.3603 - val_accuracy: 0.4506\nEpoch 16/20\n6/6 [==============================] - 1s 154ms/step - loss: 0.7331 - accuracy: 0.7553 - val_loss: 1.3652 - val_accuracy: 0.4630\nEpoch 17/20\n6/6 [==============================] - 1s 155ms/step - loss: 0.7081 - accuracy: 0.7367 - val_loss: 1.3562 - val_accuracy: 0.4630\nEpoch 18/20\n6/6 [==============================] - 1s 154ms/step - loss: 0.6940 - accuracy: 0.7872 - val_loss: 1.3810 - val_accuracy: 0.4568\nEpoch 19/20\n6/6 [==============================] - 1s 154ms/step - loss: 0.6602 - accuracy: 0.7660 - val_loss: 1.3620 - val_accuracy: 0.4630\nEpoch 20/20\n6/6 [==============================] - 1s 154ms/step - loss: 0.6422 - accuracy: 0.8218 - val_loss: 1.3928 - val_accuracy: 0.4630\n6/6 [==============================] - 1s 50ms/step\nBalanced accuracy: 0.336962481962482\n","output_type":"stream"}]},{"cell_type":"code","source":"# Saving model as .h5 file\nmodel.save(model_file)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T10:00:18.840493Z","iopub.execute_input":"2023-12-18T10:00:18.840794Z","iopub.status.idle":"2023-12-18T10:00:19.305727Z","shell.execute_reply.started":"2023-12-18T10:00:18.840767Z","shell.execute_reply":"2023-12-18T10:00:19.304927Z"},"trusted":true},"execution_count":6,"outputs":[]}]}