{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":6984590,"sourceType":"datasetVersion","datasetId":4014175}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\n\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-17T18:50:35.227442Z","iopub.execute_input":"2023-12-17T18:50:35.227697Z","iopub.status.idle":"2023-12-17T18:50:47.503286Z","shell.execute_reply.started":"2023-12-17T18:50:35.227654Z","shell.execute_reply":"2023-12-17T18:50:47.502279Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Paths\ntrain_csv = '/kaggle/input/UBC-OCEAN/train.csv'\ntrain_images = '/kaggle/input/UBC-OCEAN/train_images/'\ntrain_thumbnails = '/kaggle/input/UBC-OCEAN/train_thumbnails/'\ntrain_masks = '/kaggle/input/ubc-ovarian-cancer-competition-supplemental-masks'\nmodel_file = '/kaggle/working/model.h5'\n\nclasses = ['CC', 'EC', 'HGSC', 'LGSC', 'MC']","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:50:47.505394Z","iopub.execute_input":"2023-12-17T18:50:47.506600Z","iopub.status.idle":"2023-12-17T18:50:47.514270Z","shell.execute_reply.started":"2023-12-17T18:50:47.506562Z","shell.execute_reply":"2023-12-17T18:50:47.513346Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Functions to convert labels to one-hot vectors and vice-versa\ndef values_to_one_hot(values, classes):\n    vector = []\n    for value in values:\n        one_hot = np.zeros(5)\n        one_hot[classes.index(value)] = 1\n        vector.append(one_hot)\n    return np.array(vector)\n\ndef one_hot_to_values(vector, classes):\n    values = []\n    for one_hot in vector:\n        value = classes[np.argmax(one_hot)]\n        values.append(value)\n    return np.array(values)\n\n# Function to load image and preprocess it\ndef load_image(idx):\n    # Using TMAs and WSI thumbnails\n    try:\n        image = Image.open(train_thumbnails+str(idx)+'_thumbnail.png')\n    except:\n        image = Image.open(train_images+str(idx)+'.png')\n    # Resizing to 244x244 for Resnet model\n    image = image.resize((224,224)) \n    image = np.array(image)\n    return image\n    ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-17T18:55:58.509200Z","iopub.execute_input":"2023-12-17T18:55:58.509888Z","iopub.status.idle":"2023-12-17T18:55:58.517436Z","shell.execute_reply.started":"2023-12-17T18:55:58.509853Z","shell.execute_reply":"2023-12-17T18:55:58.516483Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Reading csv file\ntraining_df = pd.read_csv(train_csv)\nn = len(training_df)\n\n# Loading all training images \nimage_ids = []\nimages = []\nlabels = []\nfor i, (idx, label) in enumerate(zip(training_df['image_id'], training_df['label'])):\n    image = load_image(idx)\n    image_ids.append(idx)\n    images.append(image)\n    labels.append(label)\n    print(f'Loading images: {i+1} / {n}',end='\\r')\n\n# Converting labels to one-hot vectors \nlabels_one_hot = values_to_one_hot(labels, classes)\n# Reshaping image array for model training\nimages = np.array(images).reshape(-1, 224, 224, 3)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:56:01.567141Z","iopub.execute_input":"2023-12-17T18:56:01.567788Z","iopub.status.idle":"2023-12-17T18:57:55.835463Z","shell.execute_reply.started":"2023-12-17T18:56:01.567746Z","shell.execute_reply":"2023-12-17T18:57:55.834464Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Loading images: 538 / 538\r","output_type":"stream"}]},{"cell_type":"code","source":"# Loading pre-trained ResNet50 model\n# adding a final layer to change the number of output classes\nbase_model = ResNet50(weights='imagenet', include_top=False)\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\npredictions = Dense(len(classes), activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Not changing pre-trained layers\nfor layer in base_model.layers:\n  layer.trainable = False\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train-valitation split\nX_train, X_val, y_train, y_val = train_test_split(images, labels_one_hot, test_size=0.3)\n\n# Training model\nmodel.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_val, y_val))\n\n# Predicting validation set labels\npredicted = one_hot_to_values(model.predict(X_val), classes)\ny_val = one_hot_to_values(y_val, classes)\n\nprint(f'Balanced accuracy: {balanced_accuracy_score(y_val, predicted)}')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T19:10:50.286825Z","iopub.execute_input":"2023-12-17T19:10:50.287222Z","iopub.status.idle":"2023-12-17T19:11:14.960439Z","shell.execute_reply.started":"2023-12-17T19:10:50.287192Z","shell.execute_reply":"2023-12-17T19:11:14.959492Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/20\n6/6 [==============================] - 5s 360ms/step - loss: 1.7771 - accuracy: 0.3271 - val_loss: 1.6146 - val_accuracy: 0.3210\nEpoch 2/20\n6/6 [==============================] - 1s 151ms/step - loss: 1.4819 - accuracy: 0.3697 - val_loss: 1.5014 - val_accuracy: 0.3457\nEpoch 3/20\n6/6 [==============================] - 1s 152ms/step - loss: 1.3659 - accuracy: 0.4255 - val_loss: 1.4355 - val_accuracy: 0.3889\nEpoch 4/20\n6/6 [==============================] - 1s 152ms/step - loss: 1.2577 - accuracy: 0.4707 - val_loss: 1.3863 - val_accuracy: 0.3951\nEpoch 5/20\n6/6 [==============================] - 1s 151ms/step - loss: 1.1712 - accuracy: 0.5213 - val_loss: 1.3613 - val_accuracy: 0.4259\nEpoch 6/20\n6/6 [==============================] - 1s 151ms/step - loss: 1.1058 - accuracy: 0.5665 - val_loss: 1.3418 - val_accuracy: 0.4136\nEpoch 7/20\n6/6 [==============================] - 1s 152ms/step - loss: 1.0652 - accuracy: 0.5532 - val_loss: 1.3453 - val_accuracy: 0.4444\nEpoch 8/20\n6/6 [==============================] - 1s 151ms/step - loss: 0.9957 - accuracy: 0.6277 - val_loss: 1.2960 - val_accuracy: 0.4691\nEpoch 9/20\n6/6 [==============================] - 1s 152ms/step - loss: 0.9609 - accuracy: 0.6330 - val_loss: 1.3074 - val_accuracy: 0.4136\nEpoch 10/20\n6/6 [==============================] - 1s 152ms/step - loss: 0.9203 - accuracy: 0.6596 - val_loss: 1.2967 - val_accuracy: 0.4383\nEpoch 11/20\n6/6 [==============================] - 1s 151ms/step - loss: 0.8804 - accuracy: 0.6941 - val_loss: 1.2801 - val_accuracy: 0.4568\nEpoch 12/20\n6/6 [==============================] - 1s 151ms/step - loss: 0.8459 - accuracy: 0.6888 - val_loss: 1.2805 - val_accuracy: 0.4383\nEpoch 13/20\n6/6 [==============================] - 1s 152ms/step - loss: 0.8117 - accuracy: 0.7101 - val_loss: 1.2787 - val_accuracy: 0.4691\nEpoch 14/20\n6/6 [==============================] - 1s 153ms/step - loss: 0.7798 - accuracy: 0.7394 - val_loss: 1.2970 - val_accuracy: 0.4444\nEpoch 15/20\n6/6 [==============================] - 1s 152ms/step - loss: 0.7546 - accuracy: 0.7420 - val_loss: 1.2745 - val_accuracy: 0.4568\nEpoch 16/20\n6/6 [==============================] - 1s 152ms/step - loss: 0.7382 - accuracy: 0.7846 - val_loss: 1.2763 - val_accuracy: 0.4938\nEpoch 17/20\n6/6 [==============================] - 1s 151ms/step - loss: 0.7102 - accuracy: 0.7580 - val_loss: 1.2980 - val_accuracy: 0.4691\nEpoch 18/20\n6/6 [==============================] - 1s 151ms/step - loss: 0.6892 - accuracy: 0.8085 - val_loss: 1.2698 - val_accuracy: 0.4753\nEpoch 19/20\n6/6 [==============================] - 1s 151ms/step - loss: 0.6811 - accuracy: 0.7952 - val_loss: 1.2705 - val_accuracy: 0.5000\nEpoch 20/20\n6/6 [==============================] - 1s 151ms/step - loss: 0.6510 - accuracy: 0.7926 - val_loss: 1.2973 - val_accuracy: 0.4630\n6/6 [==============================] - 1s 49ms/step\nBalanced accuracy: 0.3728095268443238\n","output_type":"stream"}]},{"cell_type":"code","source":"# Saving model as .h5 file\nmodel.save(model_file)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:54:05.353872Z","iopub.execute_input":"2023-12-17T18:54:05.354180Z","iopub.status.idle":"2023-12-17T18:54:05.795079Z","shell.execute_reply.started":"2023-12-17T18:54:05.354154Z","shell.execute_reply":"2023-12-17T18:54:05.794291Z"},"trusted":true},"execution_count":6,"outputs":[]}]}