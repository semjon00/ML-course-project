{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45867,"databundleVersionId":6924515,"sourceType":"competition"},{"sourceId":7229171,"sourceType":"datasetVersion","datasetId":4185385}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\n\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-18T11:09:08.439820Z","iopub.execute_input":"2023-12-18T11:09:08.440641Z","iopub.status.idle":"2023-12-18T11:09:08.446355Z","shell.execute_reply.started":"2023-12-18T11:09:08.440604Z","shell.execute_reply":"2023-12-18T11:09:08.445456Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Paths\naug_wsi_images = '/kaggle/input/augmented/augmented/wsi'\naug_tma_images = '/kaggle/input/augmented/augmented/tma'\nmodel_file = '/kaggle/working/model.h5'\n\nclasses = ['CC', 'EC', 'HGSC', 'LGSC', 'MC']\n\n# If the best prediction probability is below that threshold, it's labelled as 'Other'\nthreshold = 0.3","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:09:08.451546Z","iopub.execute_input":"2023-12-18T11:09:08.451859Z","iopub.status.idle":"2023-12-18T11:09:08.457618Z","shell.execute_reply.started":"2023-12-18T11:09:08.451833Z","shell.execute_reply":"2023-12-18T11:09:08.456634Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# To convert labels to one-hot vectors and vice-versa\ndef values_to_one_hot(values, classes):\n    vector = []\n    for value in values:\n        one_hot = np.zeros(5)\n        one_hot[classes.index(value)] = 1\n        vector.append(one_hot)\n    return np.array(vector)\n\ndef one_hot_to_values(vector, classes):\n    values = []\n    for one_hot in vector:\n        if np.max(one_hot) < threshold:\n            value = 'Other'\n        else:\n            value = classes[np.argmax(one_hot)]\n        values.append(value)\n    return np.array(values)\n\n# To load images and labels from 'root' folder\ndef load_data(root):\n    files = os.listdir(root)\n    n = len(files)\n    \n    images = []\n    labels = []\n    confidences = []    \n\n    # Loading augmented images and labels\n    for i, file in enumerate(files):\n        if file.endswith(\".png\"):\n            image_id = os.path.splitext(file)[0]\n            label_path = os.path.join(root, f\"{image_id}.txt\")\n\n            if os.path.exists(label_path):\n                with open(label_path, \"r\") as label_file:\n                    label, confidence = label_file.read().strip().split()\n                image_path = os.path.join(root, file)\n                image = Image.open(image_path)\n                image = image.resize((224,224)) # Resizing to 244x244 for Resnet model\n                images.append(np.array(image))\n                labels.append(label)\n                confidences.append(confidence)\n        print(f'Loading images ({root}): {i+1} / {n}',end='\\r')\n    print()\n\n    # Converting labels to one-hot vectors \n    labels_one_hot = values_to_one_hot(labels, classes)\n    # Reshaping image array for model training\n    images = np.array(images).reshape(-1, 224, 224, 3)\n    \n    return images, labels_one_hot, confidences","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-18T11:09:08.459055Z","iopub.execute_input":"2023-12-18T11:09:08.459359Z","iopub.status.idle":"2023-12-18T11:09:08.473069Z","shell.execute_reply.started":"2023-12-18T11:09:08.459335Z","shell.execute_reply":"2023-12-18T11:09:08.472118Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Loading all training images \nimages_wsi, labels_wsi, confidences_wsi = load_data(aug_wsi_images)\nimages_tma, labels_tma, confidences_tma = load_data(aug_tma_images)\n\nimages = np.concatenate((images_wsi, images_tma))\nlabels_one_hot = np.concatenate((labels_wsi, labels_tma))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:09:08.474847Z","iopub.execute_input":"2023-12-18T11:09:08.475171Z","iopub.status.idle":"2023-12-18T11:09:28.165495Z","shell.execute_reply.started":"2023-12-18T11:09:08.475145Z","shell.execute_reply":"2023-12-18T11:09:28.164539Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Loading images (/kaggle/input/augmented/augmented/wsi): 1168 / 1168\nLoading images (/kaggle/input/augmented/augmented/tma): 80 / 80\n","output_type":"stream"}]},{"cell_type":"code","source":"# To test with original data\n\ntrain_csv = '/kaggle/input/UBC-OCEAN/train.csv'\ntrain_images = '/kaggle/input/UBC-OCEAN/train_images/'\ntrain_thumbnails = '/kaggle/input/UBC-OCEAN/train_thumbnails/'\n\ndef load_image(idx):\n    try:\n        image = Image.open(train_thumbnails+str(idx)+'_thumbnail.png')\n    except:\n        image = Image.open(train_images+str(idx)+'.png')\n    image = image.resize((224,224)) \n    image = np.array(image)\n    return image\n\ntraining_df = pd.read_csv(train_csv)\nn = len(training_df)\n\nimages_val = []\nlabels_val = []\n\nfor i, (idx, label) in enumerate(zip(training_df['image_id'], training_df['label'])):\n    image = load_image(idx)\n    images_val.append(image)\n    labels_val.append(label)\n    print(f'Loading images ({train_images}): {i+1} / {n}',end='\\r')\n\nimages_val = np.array(images_val).reshape(-1, 224, 224, 3)\nlabels_one_hot_val = values_to_one_hot(labels_val, classes)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:09:28.166549Z","iopub.execute_input":"2023-12-18T11:09:28.166852Z","iopub.status.idle":"2023-12-18T11:11:23.683685Z","shell.execute_reply.started":"2023-12-18T11:09:28.166826Z","shell.execute_reply":"2023-12-18T11:11:23.682727Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Loading images: 538 / 538\r","output_type":"stream"}]},{"cell_type":"code","source":"# Loading pre-trained ResNet50 model\n# adding a final layer to change the number of output classes\nbase_model = ResNet50(weights='imagenet', include_top=False)\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\npredictions = Dense(len(classes), activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Not changing pre-trained layers\nfor layer in base_model.layers:\n  layer.trainable = False\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Training model\nmodel.fit(images, labels_one_hot, epochs=20, batch_size=64, validation_data=(images_val, labels_one_hot_val))\n\n# Predicting validation set labels\npredicted = one_hot_to_values(model.predict(images_val), classes)\n\nprint(f'Balanced accuracy: {balanced_accuracy_score(labels_val, predicted)}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-12-18T11:11:23.685504Z","iopub.execute_input":"2023-12-18T11:11:23.685827Z","iopub.status.idle":"2023-12-18T11:12:09.003887Z","shell.execute_reply.started":"2023-12-18T11:11:23.685801Z","shell.execute_reply":"2023-12-18T11:12:09.002921Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch 1/20\n10/10 [==============================] - 7s 325ms/step - loss: 1.4042 - accuracy: 0.3814 - val_loss: 2.7185 - val_accuracy: 0.3457\nEpoch 2/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.9075 - accuracy: 0.6474 - val_loss: 2.7942 - val_accuracy: 0.2714\nEpoch 3/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.7390 - accuracy: 0.7147 - val_loss: 2.5647 - val_accuracy: 0.3587\nEpoch 4/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.6383 - accuracy: 0.7500 - val_loss: 2.7936 - val_accuracy: 0.3662\nEpoch 5/20\n10/10 [==============================] - 2s 189ms/step - loss: 0.5844 - accuracy: 0.7756 - val_loss: 3.0336 - val_accuracy: 0.3978\nEpoch 6/20\n10/10 [==============================] - 2s 189ms/step - loss: 0.5340 - accuracy: 0.8141 - val_loss: 2.8587 - val_accuracy: 0.3996\nEpoch 7/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.4808 - accuracy: 0.8349 - val_loss: 3.2838 - val_accuracy: 0.3885\nEpoch 8/20\n10/10 [==============================] - 2s 189ms/step - loss: 0.4428 - accuracy: 0.8526 - val_loss: 3.1643 - val_accuracy: 0.3903\nEpoch 9/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.4175 - accuracy: 0.8782 - val_loss: 3.3369 - val_accuracy: 0.3959\nEpoch 10/20\n10/10 [==============================] - 2s 191ms/step - loss: 0.3902 - accuracy: 0.8750 - val_loss: 3.5126 - val_accuracy: 0.3996\nEpoch 11/20\n10/10 [==============================] - 2s 189ms/step - loss: 0.3704 - accuracy: 0.8894 - val_loss: 3.5524 - val_accuracy: 0.3959\nEpoch 12/20\n10/10 [==============================] - 2s 191ms/step - loss: 0.3500 - accuracy: 0.9071 - val_loss: 3.4980 - val_accuracy: 0.4089\nEpoch 13/20\n10/10 [==============================] - 2s 192ms/step - loss: 0.3316 - accuracy: 0.8990 - val_loss: 3.9102 - val_accuracy: 0.4164\nEpoch 14/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.3141 - accuracy: 0.9119 - val_loss: 3.5802 - val_accuracy: 0.3996\nEpoch 15/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.3044 - accuracy: 0.9199 - val_loss: 3.8670 - val_accuracy: 0.4108\nEpoch 16/20\n10/10 [==============================] - 2s 189ms/step - loss: 0.2901 - accuracy: 0.9215 - val_loss: 3.9729 - val_accuracy: 0.4108\nEpoch 17/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.2748 - accuracy: 0.9327 - val_loss: 3.9293 - val_accuracy: 0.4108\nEpoch 18/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.2630 - accuracy: 0.9359 - val_loss: 4.1150 - val_accuracy: 0.4108\nEpoch 19/20\n10/10 [==============================] - 2s 190ms/step - loss: 0.2494 - accuracy: 0.9471 - val_loss: 4.0308 - val_accuracy: 0.4089\nEpoch 20/20\n10/10 [==============================] - 2s 189ms/step - loss: 0.2432 - accuracy: 0.9487 - val_loss: 4.1981 - val_accuracy: 0.4071\n17/17 [==============================] - 2s 50ms/step\nBalanced accuracy: 0.24237404559985204\n","output_type":"stream"}]},{"cell_type":"code","source":"# Saving model as .h5 file\nmodel.save(model_file)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T11:12:09.005310Z","iopub.execute_input":"2023-12-18T11:12:09.005693Z","iopub.status.idle":"2023-12-18T11:12:09.458670Z","shell.execute_reply.started":"2023-12-18T11:12:09.005658Z","shell.execute_reply":"2023-12-18T11:12:09.457896Z"},"trusted":true},"execution_count":47,"outputs":[]}]}